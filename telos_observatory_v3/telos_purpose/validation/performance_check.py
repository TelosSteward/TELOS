"""
Performance Analysis Tool for TELOS Observatory
===============================================

Analyzes platform performance for production readiness:
- Import time
- Data loading speed
- Analytics computation performance
- Memory usage

Usage:
    python -m telos_purpose.validation.performance_check
"""

import time
import tracemalloc
from pathlib import Path
import sys
from typing import List, Tuple


class PerformanceAnalyzer:
    """Analyze platform performance for optimization"""

    def __init__(self):
        self.results: List[Tuple[str, str, str]] = []

    def run_analysis(self):
        """Execute all performance checks"""

        print('\n‚ö° TELOS Observatory Performance Analysis')
        print('=' * 60)

        self.check_import_time()
        self.check_data_loading_speed()
        self.check_analytics_performance()
        self.check_memory_usage()
        self.check_export_performance()

        self.print_summary()

    def check_import_time(self):
        """Measure module import time"""
        print('\nüì¶ Checking Import Performance...')

        start = time.time()
        try:
            from telos_purpose.dev_dashboard import streamlit_live_comparison
            elapsed = time.time() - start

            print(f'  ‚è±  Dashboard import took {elapsed:.2f}s')

            if elapsed < 2.0:
                self.results.append(('‚úì Dashboard import time', f'{elapsed:.2f}s', 'Good'))
                print(f'  ‚úì Good performance')
            elif elapsed < 5.0:
                self.results.append(('‚ö† Dashboard import time', f'{elapsed:.2f}s', 'Acceptable'))
                print(f'  ‚ö† Acceptable performance')
            else:
                self.results.append(('‚úó Dashboard import time', f'{elapsed:.2f}s', 'Slow'))
                print(f'  ‚úó Slow - needs optimization')
        except Exception as e:
            self.results.append(('‚úó Dashboard import', 'Failed', str(e)))
            print(f'  ‚úó ERROR: {e}')

    def check_data_loading_speed(self):
        """Measure session data loading performance"""
        print('\nüíæ Checking Data Loading Speed...')

        try:
            from telos_purpose.test_data.generate_test_sessions import generate_test_session

            # Generate test session
            start = time.time()
            session = generate_test_session(50, 'perf_test')  # 50 turn session
            elapsed = time.time() - start

            print(f'  ‚è±  Generated 50-turn session in {elapsed:.3f}s')

            if elapsed < 0.5:
                self.results.append(('‚úì Session generation (50 turns)', f'{elapsed:.3f}s', 'Fast'))
                print(f'  ‚úì Fast generation')
            elif elapsed < 1.0:
                self.results.append(('‚ö† Session generation (50 turns)', f'{elapsed:.3f}s', 'Acceptable'))
                print(f'  ‚ö† Acceptable speed')
            else:
                self.results.append(('‚úó Session generation (50 turns)', f'{elapsed:.3f}s', 'Slow'))
                print(f'  ‚úó Slow - needs optimization')

            # Test turn generation rate
            turns_per_sec = 50 / elapsed
            print(f'  üìä Generation rate: {turns_per_sec:.1f} turns/second')

        except Exception as e:
            self.results.append(('‚úó Data loading', 'Failed', str(e)))
            print(f'  ‚úó ERROR: {e}')
            import traceback
            traceback.print_exc()

    def check_analytics_performance(self):
        """Measure analytics computation speed"""
        print('\nüìä Checking Analytics Performance...')

        try:
            from telos_purpose.test_data.generate_test_sessions import generate_test_suite
            from telos_purpose.dev_dashboard.streamlit_live_comparison import (
                compute_avg_fidelity_across_sessions,
                count_interventions_across_sessions
            )

            # Generate test sessions
            print('  üì¶ Generating test suite...')
            sessions = generate_test_suite()
            print(f'  ‚úì Generated {len(sessions)} sessions')

            # Measure analytics computation
            start = time.time()
            avg_fid = compute_avg_fidelity_across_sessions(sessions)
            interventions = count_interventions_across_sessions(sessions)
            elapsed = time.time() - start

            print(f'  ‚è±  Analytics computed in {elapsed:.3f}s')
            print(f'  üìà Avg fidelity: {avg_fid:.3f}, Interventions: {interventions}')

            if elapsed < 0.1:
                self.results.append(('‚úì Analytics computation', f'{elapsed:.3f}s', 'Fast'))
                print(f'  ‚úì Fast computation')
            elif elapsed < 0.5:
                self.results.append(('‚ö† Analytics computation', f'{elapsed:.3f}s', 'Acceptable'))
                print(f'  ‚ö† Acceptable speed')
            else:
                self.results.append(('‚úó Analytics computation', f'{elapsed:.3f}s', 'Slow'))
                print(f'  ‚úó Slow - needs optimization')
        except Exception as e:
            self.results.append(('‚úó Analytics performance', 'Failed', str(e)))
            print(f'  ‚úó ERROR: {e}')
            import traceback
            traceback.print_exc()

    def check_memory_usage(self):
        """Measure memory footprint"""
        print('\nüß† Checking Memory Usage...')

        try:
            tracemalloc.start()

            # Load test data
            print('  üì¶ Loading test data...')
            from telos_purpose.test_data.generate_test_sessions import generate_test_suite
            sessions = generate_test_suite()

            # Compute analytics multiple times
            print('  üîÑ Running analytics 10 times...')
            from telos_purpose.dev_dashboard.streamlit_live_comparison import (
                compute_avg_fidelity_across_sessions
            )
            for i in range(10):
                compute_avg_fidelity_across_sessions(sessions)

            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            current_mb = current / 1024 / 1024
            peak_mb = peak / 1024 / 1024

            print(f'  üìä Current memory: {current_mb:.1f} MB')
            print(f'  üìä Peak memory: {peak_mb:.1f} MB')

            if peak_mb < 50:
                self.results.append(('‚úì Peak memory usage', f'{peak_mb:.1f} MB', 'Good'))
                print(f'  ‚úì Low memory usage')
            elif peak_mb < 100:
                self.results.append(('‚ö† Peak memory usage', f'{peak_mb:.1f} MB', 'Acceptable'))
                print(f'  ‚ö† Moderate memory usage')
            else:
                self.results.append(('‚úó Peak memory usage', f'{peak_mb:.1f} MB', 'High'))
                print(f'  ‚úó High memory usage - needs optimization')
        except Exception as e:
            self.results.append(('‚úó Memory check', 'Failed', str(e)))
            print(f'  ‚úó ERROR: {e}')
            import traceback
            traceback.print_exc()

    def check_export_performance(self):
        """Measure export operation speed"""
        print('\nüì• Checking Export Performance...')

        try:
            import json
            import tempfile
            from telos_purpose.test_data.generate_test_sessions import generate_test_session

            # Generate test session
            session = generate_test_session(20, 'export_perf_test')

            # Measure JSON export
            start = time.time()
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=True) as f:
                json.dump(session, f, indent=2)
                f.flush()
            elapsed = time.time() - start

            print(f'  ‚è±  JSON export (20 turns) took {elapsed:.3f}s')

            if elapsed < 0.1:
                self.results.append(('‚úì JSON export (20 turns)', f'{elapsed:.3f}s', 'Fast'))
                print(f'  ‚úì Fast export')
            elif elapsed < 0.5:
                self.results.append(('‚ö† JSON export (20 turns)', f'{elapsed:.3f}s', 'Acceptable'))
                print(f'  ‚ö† Acceptable speed')
            else:
                self.results.append(('‚úó JSON export (20 turns)', f'{elapsed:.3f}s', 'Slow'))
                print(f'  ‚úó Slow - needs optimization')

        except Exception as e:
            self.results.append(('‚úó Export performance', 'Failed', str(e)))
            print(f'  ‚úó ERROR: {e}')

    def print_summary(self):
        """Print performance analysis summary"""
        print('\n' + '=' * 60)
        print('üìã Performance Analysis Summary')
        print('=' * 60)

        for test, value, status in self.results:
            status_icon = '‚úì' if status in ['Good', 'Fast'] else ('‚ö†' if status == 'Acceptable' else '‚úó')
            print(f'{status_icon} {test}: {value} ({status})')

        print('\n' + '=' * 60)

        # Overall assessment
        failures = sum(1 for _, _, s in self.results if '‚úó' in str(s) or s == 'Failed')
        warnings = sum(1 for _, _, s in self.results if '‚ö†' in str(s) or s == 'Acceptable')
        successes = sum(1 for _, _, s in self.results if '‚úì' in str(s) or s in ['Good', 'Fast'])

        total = len(self.results)
        print(f'\nüìä Results: {successes} Good, {warnings} Acceptable, {failures} Need Optimization')
        print(f'   Total Checks: {total}')

        if failures == 0 and warnings == 0:
            print('\n‚ú® Excellent performance! Platform is production-ready.')
            sys.exit(0)
        elif failures == 0:
            print(f'\n‚ö†Ô∏è  Performance acceptable with {warnings} warnings.')
            print('   Consider optimization for best user experience.')
            sys.exit(0)
        else:
            print(f'\n‚ö†Ô∏è  Performance needs optimization: {failures} issues found.')
            print('   Review slow operations and apply optimizations.')
            sys.exit(1)


def main():
    """Main entry point for performance analysis"""
    analyzer = PerformanceAnalyzer()
    analyzer.run_analysis()


if __name__ == '__main__':
    main()
